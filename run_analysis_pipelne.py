import re
import underthesea
import pickle
import pandas as pd
import random
import numpy as np
from datetime import datetime, timedelta
from tqdm import tqdm
from sklearn.feature_extraction.text import TfidfVectorizer
from gensim.models import Word2Vec
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from imblearn.over_sampling import SMOTE
from data_preprocessing.feature_engineering import FeatureEngineer
from database.connection import get_db_connection

# ============================
# 1ï¸âƒ£ Khá»Ÿi táº¡o FeatureEngineer Ä‘á»ƒ xá»­ lÃ½ vector hÃ³a
# ============================
featureEngineer = FeatureEngineer()

# ============================
# 2ï¸âƒ£ Káº¿t ná»‘i Database
# ============================


def fetch_reviews():
    """Láº¥y dá»¯ liá»‡u review tá»« database."""
    print("ğŸ“¡ Äang láº¥y dá»¯ liá»‡u tá»« database...")
    conn = get_db_connection()
    query = "SELECT review_id, final_text, processed_at AS created_at FROM processed_reviews"
    df = pd.read_sql(query, conn)
    conn.close()
    print(f"âœ… Láº¥y thÃ nh cÃ´ng {len(df)} dÃ²ng dá»¯ liá»‡u tá»« database.")
    return df


# ============================
# 3ï¸âƒ£ Load mÃ´ hÃ¬nh PhoBERT Ä‘á»ƒ phÃ¢n tÃ­ch cáº£m xÃºc tiáº¿ng Viá»‡t
# ============================
model_name = "vinai/phobert-base"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForSequenceClassification.from_pretrained(model_name)
sentiment_model = pipeline(
    "sentiment-analysis", model=model, tokenizer=tokenizer)

# ============================
# 4ï¸âƒ£ Thiáº¿t láº­p TF-IDF vá»›i stopwords tiáº¿ng Viá»‡t
# ============================
vietnamese_stopwords = [
    "vÃ ", "lÃ ", "cÃ³", "cá»§a", "cho", "Ä‘Æ°á»£c", "ráº¥t", "vá»›i", "táº¡i", "nhÆ°ng", "thÃ¬", "khÃ´ng", "cÅ©ng",
    "Ä‘Ã¢y", "Ä‘Ã³", "Ä‘áº¥y", "vÃ¬", "sau", "trÆ°á»›c", "tá»«", "trong", "ra", "láº¡i", "nÃ y", "kia", "áº¥y",
    "báº¡n", "tÃ´i", "anh", "chá»‹", "em", "bÃªn", "kia", "váº­y"
]

tfidf_vectorizer = TfidfVectorizer(
    max_features=500,
    stop_words=vietnamese_stopwords,
    ngram_range=(1, 2),
    smooth_idf=True,
    sublinear_tf=True
)

# Tá»« Ä‘iá»ƒn khÃ­a cáº¡nh vÃ  tá»« khÃ³a liÃªn quan
aspect_keywords = {
    "phÃ²ng": ["phÃ²ng", "giÆ°á»ng", "rá»™ng rÃ£i", "sáº¡ch sáº½", "thoáº£i mÃ¡i"],
    "nhÃ¢n viÃªn": ["nhÃ¢n viÃªn", "lá»… tÃ¢n", "phá»¥c vá»¥", "chuyÃªn nghiá»‡p", "thÃ¢n thiá»‡n", "nhiá»‡t tÃ¬nh"],
    "Ä‘á»“ Äƒn": ["Ä‘á»“ Äƒn", "bá»¯a sÃ¡ng", "mÃ³n Äƒn", "nhÃ  hÃ ng", "thá»±c Ä‘Æ¡n", "khÃ´ng ngon"],
    "thá»©c Äƒn": ["Ä‘á»“ Äƒn", "bá»¯a sÃ¡ng", "mÃ³n Äƒn", "nhÃ  hÃ ng", "thá»±c Ä‘Æ¡n", "khÃ´ng ngon"],
    "vá»‹ trÃ­": ["vá»‹ trÃ­", "gáº§n biá»ƒn", "trung tÃ¢m", "thuáº­n tiá»‡n", "Ä‘i láº¡i"],
    "giÃ¡ cáº£": ["giÃ¡", "giÃ¡ cáº£", "há»£p lÃ½", "quÃ¡ Ä‘áº¯t", "ráº»"],
    "view": ["view", "cáº£nh Ä‘áº¹p", "hÆ°á»›ng biá»ƒn", "táº§m nhÃ¬n"],
    "wifi": ["wifi", "máº¡ng", "internet", "káº¿t ná»‘i"],
    "há»“ bÆ¡i": ["há»“ bÆ¡i", "bá»ƒ bÆ¡i", "nÆ°á»›c sáº¡ch"],
    "bÃ£i Ä‘á»— xe": ["bÃ£i Ä‘á»— xe", "Ä‘áº­u xe", "gá»­i xe"],
    "dá»‹ch vá»¥ phÃ²ng": ["dá»‹ch vá»¥ phÃ²ng", "room service", "phá»¥c vá»¥ táº­n phÃ²ng"],
    "spa": ["spa", "massage", "trá»‹ liá»‡u"],
    "gym": ["gym", "phÃ²ng táº­p", "táº­p thá»ƒ dá»¥c"],
    "giáº£i trÃ­": ["giáº£i trÃ­", "karaoke", "ráº¡p chiáº¿u phim"],
    "Ä‘Æ°a Ä‘Ã³n": ["Ä‘Æ°a Ä‘Ã³n", "xe Ä‘Æ°a Ä‘Ã³n", "shuttle bus"],
    "bÃ£i biá»ƒn": ["bÃ£i biá»ƒn", "bien", "biá»ƒn", "nÆ°á»›c trong"],
    "nÃºi": ["nÃºi", "phong cáº£nh nÃºi", "leo nÃºi"],
    "sÃ´ng": ["sÃ´ng", "ven sÃ´ng"],
    "há»“": ["há»“", "há»“ nÆ°á»›c"],
    "Ä‘áº£o": ["Ä‘áº£o", "quáº§n Ä‘áº£o"],
    "trung tÃ¢m": ["trung tÃ¢m", "khu vá»±c trung tÃ¢m"],
    "an ninh": ["an ninh", "báº£o vá»‡", "an toÃ n"],
    "sáº¡ch sáº½": ["sáº¡ch sáº½", "gá»n gÃ ng"],
    "tiá»‡n nghi": ["tiá»‡n nghi", "Ä‘áº§y Ä‘á»§ thiáº¿t bá»‹"],
    "khÃ´ng gian": ["khÃ´ng gian", "thoÃ¡ng mÃ¡t", "rá»™ng lá»›n"],
    "yÃªn tÄ©nh": ["yÃªn tÄ©nh", "khÃ´ng á»“n Ã o"],
    "thá»i tiáº¿t": ["thá»i tiáº¿t", "khÃ­ háº­u"],
    "Ä‘á»“ uá»‘ng": ["Ä‘á»“ uá»‘ng", "quáº§y bar", "cocktail"],
    "tráº£i nghiá»‡m tá»•ng thá»ƒ": ["tráº£i nghiá»‡m", "tá»•ng thá»ƒ", "cáº£m giÃ¡c"]
}

# Táº¡o tá»« Ä‘iá»ƒn descriptive_words cho cÃ¡c tá»« mÃ´ táº£
descriptive_words = {
    "phÃ²ng": ["rá»™ng", "thoáº£i mÃ¡i", "sáº¡ch", "gá»n gÃ ng", "thoÃ¡ng", "Ä‘áº¹p"],
    "nhÃ¢n viÃªn": ["chuyÃªn nghiá»‡p", "thÃ¢n thiá»‡n", "nhiá»‡t tÃ¬nh", "tá»‘t", "tá»‡"],
    "Ä‘á»“ Äƒn": ["ngon", "dá»Ÿ", "tÆ°Æ¡i", "máº·n", "nháº¡t"],
    "vá»‹ trÃ­": ["tá»‘t", "gáº§n", "tiá»‡n lá»£i", "xa", "dá»… dÃ ng", "thuáº­n tiá»‡n"],
    "giÃ¡ cáº£": ["há»£p lÃ½", "Ä‘áº¯t", "ráº»", "pháº£i chÄƒng"],
    "view": ["Ä‘áº¹p", "rá»™ng", "hÆ°á»›ng biá»ƒn", "tuyá»‡t vá»i", "háº¥p dáº«n"],
    "wifi": ["máº¡nh", "kÃ©m", "á»•n Ä‘á»‹nh", "cháº­m"],
    "há»“ bÆ¡i": ["sáº¡ch", "mÃ¡t", "láº¡nh", "thoáº£i mÃ¡i"],
    "bÃ£i Ä‘á»— xe": ["dá»… dÃ ng", "thoáº£i mÃ¡i", "cháº­t", "khÃ³ khÄƒn"],
    "dá»‹ch vá»¥ phÃ²ng": ["tá»‘t", "tuyá»‡t vá»i", "kÃ©m", "nhanh", "cháº­m"],
    "spa": ["thÆ° giÃ£n", "tuyá»‡t vá»i", "kÃ©m", "tá»‘t"],
    "gym": ["hiá»‡n Ä‘áº¡i", "Ä‘áº§y Ä‘á»§", "khÃ´ng gian rá»™ng", "cÆ¡ báº£n"],
    "giáº£i trÃ­": ["vui", "thÃº vá»‹", "buá»“n táº»", "nhÃ m chÃ¡n"],
    "Ä‘Æ°a Ä‘Ã³n": ["tiá»‡n lá»£i", "nhanh chÃ³ng", "tá»‘t"],
    "bÃ£i biá»ƒn": ["sáº¡ch", "cÃ¡t tráº¯ng", "nÆ°á»›c trong", "mÃ¡t", "Ä‘áº¹p"],
    "nÃºi": ["cao", "hÃ¹ng vÄ©", "Ä‘áº¹p"],
    "sÃ´ng": ["trong", "mÃ¡t", "háº¥p dáº«n"],
    "há»“": ["sáº¡ch", "trong", "mÃ¡t"],
    "Ä‘áº£o": ["hoang sÆ¡", "Ä‘áº¹p", "tuyá»‡t vá»i"],
    "trung tÃ¢m": ["ná»•i báº­t", "táº¥p náº­p", "sÃ´i Ä‘á»™ng"],
    "an ninh": ["an toÃ n", "cháº·t cháº½", "kÃ©m"],
    "sáº¡ch sáº½": ["gá»n gÃ ng", "ngÄƒn náº¯p", "má»› há»—n Ä‘á»™n"],
    "tiá»‡n nghi": ["Ä‘áº§y Ä‘á»§", "hiá»‡n Ä‘áº¡i", "kÃ©m"],
    "khÃ´ng gian": ["rá»™ng", "thoÃ¡ng", "háº¹p"],
    "yÃªn tÄ©nh": ["thoáº£i mÃ¡i", "yÃªn bÃ¬nh", "á»“n Ã o"],
    "thá»i tiáº¿t": ["náº¯ng", "mÆ°a", "láº¡nh", "nÃ³ng"],
    "Ä‘á»“ uá»‘ng": ["ngon", "mÃ¡t", "khÃ³ uá»‘ng"],
    "tráº£i nghiá»‡m tá»•ng thá»ƒ": ["tuyá»‡t vá»i", "tá»‘t", "kÃ©m", "tá»‡"]
}


# Chuáº©n hÃ³a tá»«: bá» dáº¥u cÃ¢u, chuyá»ƒn thÃ nh _ náº¿u lÃ  tá»« ghÃ©p, nhÆ°ng khÃ´ng cÃ³ dáº¥u _ á»Ÿ Ä‘áº§u

def normalize_token(word):
    word = re.sub(r'[^\w\s]', '', word)  # bá» dáº¥u cÃ¢u
    word = word.strip()
    word = word.replace(" ", "_")
    if word.startswith("_"):
        word = word[1:]
    return word.lower()


def analyze_aspect_sentiments(text):
    """PhÃ¢n tÃ­ch cÃ¡c khÃ­a cáº¡nh trong cÃ¢u vÃ  trÃ­ch xuáº¥t tá»« khÃ³a mÃ´ táº£ & ngá»¯ cáº£nh chÃ­nh xÃ¡c theo tá»«ng aspect."""
    tokens = underthesea.word_tokenize(text, format="text").split()
    token_text = "_".join(tokens)  # Chuáº©n hÃ³a Ä‘á»ƒ so sÃ¡nh keyword dá»… hÆ¡n

    detected_aspects = {}

    for aspect, keywords in aspect_keywords.items():
        for keyword in keywords:
            norm_keyword = keyword.replace(" ", "_")
            if norm_keyword in token_text:
                # TÃ¬m vá»‹ trÃ­ keyword trong tokens
                positions = [i for i, t in enumerate(
                    tokens) if norm_keyword in t or keyword in t]
                for pos in positions:
                    detected_aspects.setdefault(aspect, set())

                    # Láº¥y ngá»¯ cáº£nh xung quanh keyword Â±4 tá»«
                    start = max(0, pos - 4)
                    end = min(len(tokens), pos + 5)
                    context = tokens[start:end]

                    # Táº­p há»£p tá»« khÃ³a liÃªn quan trong ngá»¯ cáº£nh
                    context_keywords = set()
                    context_keywords.add(aspect.replace(" ", "_"))
                    context_keywords.add(norm_keyword)

                    for word in context:
                        norm_word = normalize_token(word)
                        if not norm_word or norm_word in vietnamese_stopwords:
                            continue
                        if norm_word in descriptive_words.get(aspect, []):
                            context_keywords.add(norm_word)
                        elif norm_word != aspect.replace(" ", "_"):
                            context_keywords.add(norm_word)

                    # Cáº­p nháº­t tá»« khÃ³a theo tá»«ng context riÃªng biá»‡t
                    detected_aspects[aspect].update(context_keywords)

    if not detected_aspects:
        return []

    sentiments = []
    for aspect, extracted_keywords in detected_aspects.items():
        sentiment_score = round(random.uniform(-1, 1), 2)
        confidence = round(random.uniform(0.7, 1.0), 2)

        rule_based_keywords = {
            "sáº¡ch_sáº½", "rá»™ng_rÃ£i", "nhÃ¢n_viÃªn", "chuyÃªn_nghiá»‡p", "Ä‘á»“_Äƒn", "ngon", "giÃ¡", "cao",
            "tuyá»‡t_vá»i", "xuáº¥t_sáº¯c", "hoÃ n_háº£o", "thÃ¢n_thiá»‡n", "chu_Ä‘Ã¡o", "thoáº£i_mÃ¡i", "an_toÃ n",
            "yÃªn_tÄ©nh", "thuáº­n_tiá»‡n", "Ä‘áº¹p", "háº¥p_dáº«n", "phong_phÃº", "Ä‘a_dáº¡ng", "giÃ¡_cáº£_há»£p_lÃ½"
        }

        if any(word in rule_based_keywords for word in extracted_keywords):
            extraction_method = "rule-based"
        else:
            extraction_method = "machine-learning"

        sentiments.append((
            aspect,
            sentiment_score,
            confidence,
            extracted_keywords,
            extraction_method
        ))

    return sentiments


# ============================
# 5ï¸âƒ£ Xá»­ lÃ½ tá»«ng bÃ i Ä‘Ã¡nh giÃ¡
# ============================


def process_reviews(df):
    """Xá»­ lÃ½ dá»¯ liá»‡u review, táº¡o Ä‘áº·c trÆ°ng TF-IDF vÃ  Word2Vec, phÃ¢n tÃ­ch cáº£m xÃºc."""
    print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh xá»­ lÃ½ reviews...")
    results = []
    aspect_sentiments = []
    all_texts = df["final_text"].tolist()
    tfidf_vectorizer.fit(all_texts)
    if not featureEngineer.word2vec_model:
        featureEngineer.create_word_embeddings(all_texts, retrain=True)

    for index, row in tqdm(df.iterrows(), total=len(df), desc="ğŸ“ Xá»­ lÃ½ reviews"):
        review_id, text, created_at = row['review_id'], row['final_text'], row['created_at']

        # Generate the TF-IDF features for this review
        tfidf_features = tfidf_vectorizer.transform([text]).toarray()[0]

        # Generate Word2Vec embeddings for this review
        word2vec_embeddings = featureEngineer.create_word_embeddings(text)

        # Ensure word2vec_embeddings is a numpy array 1D
        if isinstance(word2vec_embeddings, list) and len(word2vec_embeddings) == 1:
            word2vec_embeddings = word2vec_embeddings[0]

        # Add results to the list
        results.extend([
            (review_id, "tfidf", tfidf_features,
             tfidf_features.shape[0], created_at),
            (review_id, "word2vec", word2vec_embeddings,
             word2vec_embeddings.shape[0], created_at)
        ])

        # Process aspect sentiments
        aspects = analyze_aspect_sentiments(text)
        for aspect, sentiment_score, confidence, extracted_keywords, extraction_method in aspects:
            aspect_sentiments.append((review_id, aspect, sentiment_score, confidence,
                                     extracted_keywords, extraction_method, created_at))

    print("âœ… Xá»­ lÃ½ review hoÃ n táº¥t!")
    return results, aspect_sentiments

# ============================
# 6ï¸âƒ£ LÆ°u dá»¯ liá»‡u vÃ o database
# ============================


def save_to_database(results, aspect_sentiments):
    """LÆ°u dá»¯ liá»‡u embeddings vÃ  aspect sentiments vÃ o database."""
    conn = get_db_connection()
    cursor = conn.cursor()
    print("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u vÃ o database...")
    for review_id, embedding_type, embedding, dimensions, created_at in results:
        # Äáº£m báº£o embedding lÃ  numpy array
        if not isinstance(embedding, np.ndarray):
            embedding = np.array(embedding)

        # Äáº£m báº£o dimensions khá»›p vá»›i kÃ­ch thÆ°á»›c thá»±c
        actual_dimensions = embedding.size
        if dimensions != actual_dimensions:
            print(
                f"âš ï¸ Cáº£nh bÃ¡o: KhÃ´ng khá»›p kÃ­ch thÆ°á»›c cho review {review_id}, type {embedding_type}: Expected {dimensions}, got {actual_dimensions}")
            dimensions = actual_dimensions

        embedding_bytes = pickle.dumps(embedding)
        cursor.execute("""
            INSERT INTO review_embeddings (review_id, embedding_type, embedding, dimensions, created_at)
            VALUES (%s, %s, %s, %s, %s);
        """, (review_id, embedding_type, embedding_bytes, dimensions, created_at))
    for review_id, aspect, sentiment_score, confidence, extracted_keywords, extraction_method, created_at in aspect_sentiments:
        cursor.execute("""
            INSERT INTO aspect_sentiments (review_id, aspect, sentiment_score, confidence, extracted_keywords, extraction_method, created_at)
            VALUES (%s, %s, %s, %s, %s, %s, %s);
        """, (review_id, aspect, sentiment_score, confidence, list(extracted_keywords), extraction_method, created_at))
    conn.commit()
    cursor.close()
    conn.close()
    print("âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o database!")

# ============================
# 9ï¸âƒ£ LÆ°u dá»¯ liá»‡u vÃ o file Excel
# ============================


def save_to_excel(results, aspect_sentiments, filename="output.xlsx"):
    """LÆ°u dá»¯ liá»‡u embeddings vÃ  aspect sentiments vÃ o file Excel."""
    print("ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u vÃ o Excel...")
    df_results = pd.DataFrame(results, columns=[
                              "review_id", "embedding_type", "embedding", "dimensions", "created_at"])
    df_aspects = pd.DataFrame(aspect_sentiments, columns=[
                              "review_id", "aspect", "sentiment_score", "confidence", "extracted_keywords", "extraction_method", "created_at"])

    with pd.ExcelWriter(filename) as writer:
        df_results.to_excel(
            writer, sheet_name="Review Embeddings", index=False)
        df_aspects.to_excel(
            writer, sheet_name="Aspect Sentiments", index=False)

    print(f"âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o {filename}!")

# ============================
# ğŸ§ª TEST ÄOáº N VÄ‚N MáºªU
# ============================


test_text = """â€œHomestay cháº¥t lÆ°á»£ngâ€ PhÃ²ng á»Ÿ sáº¡ch sáº½, tuy nhiÃªn diá»‡n tÃ­ch hÆ¡i nhá», bÃ¹ láº¡i bÃ y trÃ­ xinh. 
NhÃ¢n viÃªn lá»‹ch sá»±. Trang thiáº¿t bá»‹ cÅ©ng ok nhÆ°ng khÃ´ng cÃ³ tá»§ láº¡nh cÅ©ng hÆ¡i báº¥t tiá»‡n. NhÆ°ng nhÃ¬n chung thÃ¬ khÃ¡ á»•n"""

print("\nğŸ” Äang phÃ¢n tÃ­ch Ä‘oáº¡n vÄƒn máº«u:\n", test_text)
print("\nğŸ“ Káº¿t quáº£ trÃ­ch xuáº¥t:")
result = analyze_aspect_sentiments(test_text)
for aspect, sentiment_score, confidence, extracted_keywords, extraction_method in result:
    print(f"\nğŸ”¹ Aspect: {aspect}")
    print(f"   ğŸ”¸ Sentiment Score: {sentiment_score}")
    print(f"   ğŸ”¸ Confidence: {confidence}")
    print(f"   ğŸ”¸ Extracted Keywords: {extracted_keywords}")
    print(f"   ğŸ”¸ Extraction Method: {extraction_method}")


# ============================
# ğŸ”¥ CHáº Y CHÆ¯Æ NG TRÃŒNH CHÃNH
# ============================
if __name__ == "__main__":
    df_reviews = fetch_reviews()
    results, aspect_sentiments = process_reviews(df_reviews)
    save_to_excel(results, aspect_sentiments)
    save_to_database(results, aspect_sentiments)
    print("ğŸ‰ HoÃ n thÃ nh quÃ¡ trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u!")
