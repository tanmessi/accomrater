# app_updated.py - Updated Streamlit App with new ACSA model logic

import streamlit as st
import sys
import os
import pandas as pd
from typing import List, Dict
from tensorflow.data import Dataset
from tensorflow.keras.optimizers import Adam

# Project imports
from processors.vlsp2018_processor import VLSP2018Loader, PolarityMapping
from processors.vietnamese_processor import VietnameseTextPreprocessor
from transformers import AutoTokenizer
from acsa_model import VLSP2018MultiTask

# Constants
PRETRAINED_MODEL = 'vinai/phobert-base'
MAX_LENGTH = 256
WEIGHTS_DIR = './weights'

# Streamlit page config
st.set_page_config(
    page_title="Ph√¢n t√≠ch c·∫£m x√∫c review kh√°ch s·∫°n",
    page_icon="üè®",
    layout="wide"
)

# Cache resources
@st.cache_resource
def load_tokenizer_and_preprocessor():
    """Load tokenizer and Vietnamese preprocessor"""
    tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)
    vn_preprocessor = VietnameseTextPreprocessor(
        vncorenlp_dir='processors/VnCoreNLP',
        extra_teencodes={
            'kh√°ch s·∫°n': ['ks'], 'nh√† h√†ng': ['nhahang'], 'nh√¢n vi√™n': ['nv'],
            'c·ª≠a h√†ng': ['store', 'sop', 'shopE', 'shop'],
            's·∫£n ph·∫©m': ['sp', 'product'], 'h√†ng': ['h√†g'],
            'giao h√†ng': ['ship', 'delivery', 's√≠p'], 'ƒë·∫∑t h√†ng': ['order'],
            'chu·∫©n ch√≠nh h√£ng': ['authentic', 'aut', 'auth'], 'h·∫°n s·ª≠ d·ª•ng': ['date', 'hsd'],
            'ƒëi·ªán tho·∫°i': ['dt'], 'facebook': ['fb', 'face'],
            'nh·∫Øn tin': ['nt', 'ib'], 'tr·∫£ l·ªùi': ['tl', 'trl', 'rep'],
            'feedback': ['fback', 'fedback'], 's·ª≠ d·ª•ng': ['sd'], 'x√†i': ['s√†i'],
        },
        max_correction_length=MAX_LENGTH
    )
    return tokenizer, vn_preprocessor

@st.cache_resource
def get_aspect_category_names():
    """Get aspect category names from dataset"""
    TRAIN_PATH = r'./datasets/vlsp2018_hotel/1-VLSP2018-SA-Hotel-train.csv'
    VAL_PATH = r'./datasets/vlsp2018_hotel/2-VLSP2018-SA-Hotel-dev.csv'
    TEST_PATH = r'./datasets/vlsp2018_hotel/3-VLSP2018-SA-Hotel-test.csv'

    raw_datasets = VLSP2018Loader.load(TRAIN_PATH, VAL_PATH, TEST_PATH)
    return raw_datasets['train'].column_names[1:]

@st.cache_resource
def load_model(model_path: str, aspect_category_names: List[str], _tokenizer):
    """Load ACSA model with weights - following test_app_logic.py structure exactly"""
    import tensorflow as tf

    # Extract model name from path
    model_name = os.path.basename(model_path).replace('.h5', '')

    # Create optimizer (same as test_app_logic.py line 54)
    optimizer = Adam(learning_rate=1e-4)

    # Instantiate model (same as test_app_logic.py line 55)
    # Do NOT use multi_branch parameter - use default
    model = VLSP2018MultiTask(PRETRAINED_MODEL, aspect_category_names, optimizer, name=model_name)

    try:
        # Build the model first with a dummy input (same as test_app_logic.py line 58-68)
        dummy_inputs = _tokenizer(
            "dummy text",
            max_length=MAX_LENGTH,
            padding='max_length',
            truncation=True,
            return_tensors='tf'
        )

        # Call the model to build it - convert BatchEncoding to dict
        _ = model(dict(dummy_inputs))

        # Load the weights with by_name and skip_mismatch (CRITICAL - line 71)
        model.load_weights(model_path, by_name=True, skip_mismatch=True)
        return model, None

    except Exception as e:
        return None, str(e)

def get_available_models() -> List[str]:
    """Get list of available model weights"""
    if not os.path.exists(WEIGHTS_DIR):
        return []

    models = []
    for item in os.listdir(WEIGHTS_DIR):
        if item.endswith('.h5') or os.path.isdir(os.path.join(WEIGHTS_DIR, item)):
            models.append(item)

    return sorted(models)

def analyze_single_review(review_text: str, model, tokenizer, vn_preprocessor, aspect_category_names) -> Dict:
    """
    Analyze a single review and return predictions

    Returns:
        Dict with keys: 'aspects', 'overall_sentiment', 'sentiment_score'
    """
    # Preprocess and tokenize
    processed_input = VLSP2018Loader.preprocess_and_tokenize(
        review_text, vn_preprocessor, tokenizer,
        batch_size=1, max_length=MAX_LENGTH
    )

    # Create TensorFlow dataset
    tf_inputs = Dataset.from_tensor_slices({
        x: [[processed_input[x][0]]] for x in tokenizer.model_input_names
    })

    # Predict
    predictions = model.acsa_predict(tf_inputs)

    # Parse results
    aspects = []
    polarities_map = {0: 'neutral', 1: 'positive', 2: 'negative', 3: 'none'}

    for aspect_category, polarity_idx in zip(aspect_category_names, predictions[0]):
        polarity = PolarityMapping.INDEX_TO_POLARITY.get(polarity_idx, 'none')
        if polarity and polarity != 'none':
            aspects.append({
                'aspect': aspect_category,
                'polarity': polarity,
                'polarity_idx': polarity_idx
            })

    # Calculate overall sentiment score
    sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
    for aspect in aspects:
        if aspect['polarity'] in sentiment_counts:
            sentiment_counts[aspect['polarity']] += 1

    total = sum(sentiment_counts.values())
    if total > 0:
        sentiment_score = (sentiment_counts['positive'] - sentiment_counts['negative']) / total

        if sentiment_score > 0.3:
            overall_sentiment = 'T√≠ch c·ª±c'
        elif sentiment_score < -0.3:
            overall_sentiment = 'Ti√™u c·ª±c'
        else:
            overall_sentiment = 'Trung b√¨nh'
    else:
        sentiment_score = 0
        overall_sentiment = 'Kh√¥ng x√°c ƒë·ªãnh'

    return {
        'aspects': aspects,
        'overall_sentiment': overall_sentiment,
        'sentiment_score': sentiment_score,
        'sentiment_counts': sentiment_counts
    }

def display_single_analysis_results(results: Dict, use_expander: bool = True):
    """Display results for single review analysis

    Args:
        results: Analysis results dictionary
        use_expander: If False, display aspects without nested expanders (for use inside other expanders)
    """
    # Overall sentiment
    st.markdown("### üìä K·∫øt qu·∫£ ph√¢n t√≠ch t·ªïng th·ªÉ")

    col1, col2, col3 = st.columns(3)

    with col1:
        sentiment_color = {
            'T√≠ch c·ª±c': '#00c853',
            'Ti√™u c·ª±c': '#d50000',
            'Trung b√¨nh': '#ffc107',
            'Kh√¥ng x√°c ƒë·ªãnh': '#9e9e9e'
        }.get(results['overall_sentiment'], '#9e9e9e')

        st.markdown(f"""
        <div style='padding: 20px; border-radius: 10px; background-color: {sentiment_color}20; border: 2px solid {sentiment_color};'>
            <h3 style='color: {sentiment_color}; margin: 0;'>{results['overall_sentiment']}</h3>
            <p style='margin: 5px 0 0 0; color: #666;'>C·∫£m x√∫c t·ªïng th·ªÉ</p>
        </div>
        """, unsafe_allow_html=True)

    with col2:
        st.metric("ƒêi·ªÉm c·∫£m x√∫c", f"{results['sentiment_score']:.2f}")

    with col3:
        st.markdown("**Ph√¢n b·ªë:**")
        st.write(f"‚úÖ T√≠ch c·ª±c: {results['sentiment_counts']['positive']}")
        st.write(f"‚ùå Ti√™u c·ª±c: {results['sentiment_counts']['negative']}")
        st.write(f"‚ûñ Trung b√¨nh: {results['sentiment_counts']['neutral']}")

    # Aspects details
    st.markdown("### üîç Chi ti·∫øt theo t·ª´ng kh√≠a c·∫°nh")

    if results['aspects']:
        # Group by category
        categories = {}
        for aspect in results['aspects']:
            category = aspect['aspect'].split('#')[0]
            if category not in categories:
                categories[category] = []
            categories[category].append(aspect)

        # Display by category
        for category, aspects_list in categories.items():
            if use_expander:
                # Use expander when displaying standalone (single review mode)
                with st.expander(f"**{category}** ({len(aspects_list)} kh√≠a c·∫°nh)", expanded=True):
                    for aspect in aspects_list:
                        polarity_emoji = {
                            'positive': '‚úÖ',
                            'negative': '‚ùå',
                            'neutral': '‚ûñ'
                        }.get(aspect['polarity'], '‚ùì')

                        polarity_color = {
                            'positive': '#00c853',
                            'negative': '#d50000',
                            'neutral': '#ffc107'
                        }.get(aspect['polarity'], '#9e9e9e')

                        st.markdown(f"""
                        <div style='padding: 10px; margin: 5px 0; border-left: 3px solid {polarity_color}; background-color: {polarity_color}15;'>
                            {polarity_emoji} <strong>{aspect['aspect'].split('#')[1]}</strong>: <span style='color: {polarity_color};'>{aspect['polarity']}</span>
                        </div>
                        """, unsafe_allow_html=True)
            else:
                # Display without expander (when already inside an expander)
                st.markdown(f"**{category}** ({len(aspects_list)} kh√≠a c·∫°nh)")
                for aspect in aspects_list:
                    polarity_emoji = {
                        'positive': '‚úÖ',
                        'negative': '‚ùå',
                        'neutral': '‚ûñ'
                    }.get(aspect['polarity'], '‚ùì')

                    polarity_color = {
                        'positive': '#00c853',
                        'negative': '#d50000',
                        'neutral': '#ffc107'
                    }.get(aspect['polarity'], '#9e9e9e')

                    st.markdown(f"""
                    <div style='padding: 10px; margin: 5px 0; border-left: 3px solid {polarity_color}; background-color: {polarity_color}15;'>
                        {polarity_emoji} <strong>{aspect['aspect'].split('#')[1]}</strong>: <span style='color: {polarity_color};'>{aspect['polarity']}</span>
                    </div>
                    """, unsafe_allow_html=True)
    else:
        st.info("Kh√¥ng ph√°t hi·ªán kh√≠a c·∫°nh n√†o trong review")

    # Conclusion
    st.markdown("### üìù K·∫øt lu·∫≠n")
    if results['overall_sentiment'] == 'T√≠ch c·ª±c':
        st.success(f"Review n√†y th·ªÉ hi·ªán c·∫£m x√∫c t√≠ch c·ª±c v·ªõi {results['sentiment_counts']['positive']} kh√≠a c·∫°nh ƒë∆∞·ª£c ƒë√°nh gi√° cao.")
    elif results['overall_sentiment'] == 'Ti√™u c·ª±c':
        st.error(f"Review n√†y th·ªÉ hi·ªán c·∫£m x√∫c ti√™u c·ª±c v·ªõi {results['sentiment_counts']['negative']} kh√≠a c·∫°nh c·∫ßn c·∫£i thi·ªán.")
    else:
        st.warning("Review n√†y c√≥ c·∫£m x√∫c trung b√¨nh ho·∫∑c kh√¥ng r√µ r√†ng.")

def main():
    st.title("üè® Ph√¢n t√≠ch c·∫£m x√∫c review kh√°ch s·∫°n v·ªõi ACSA")

    # Sidebar
    st.sidebar.header("‚öôÔ∏è C√†i ƒë·∫∑t")

    # Model selection
    available_models = get_available_models()
    if not available_models:
        st.sidebar.error("Kh√¥ng t√¨m th·∫•y model weights trong folder ./weights/")
        st.error("Vui l√≤ng ki·ªÉm tra folder weights/")
        return

    selected_model = st.sidebar.selectbox(
        "Ch·ªçn m√¥ h√¨nh",
        available_models,
        help="Ch·ªçn model ƒë√£ ƒë∆∞·ª£c train ƒë·ªÉ ph√¢n t√≠ch"
    )

    # Build correct model path
    if selected_model.endswith('.h5'):
        model_path = os.path.join(WEIGHTS_DIR, selected_model)
    else:
        # For folder-based models, look for model files inside
        model_path = os.path.join(WEIGHTS_DIR, selected_model, selected_model)

    # Load resources
    with st.spinner("ƒêang t·∫£i model v√† preprocessor..."):
        tokenizer, vn_preprocessor = load_tokenizer_and_preprocessor()
        aspect_category_names = get_aspect_category_names()
        model, error = load_model(model_path, aspect_category_names, tokenizer)

    if error:
        st.sidebar.error(f"L·ªói t·∫£i model: {error}")
        st.error("Kh√¥ng th·ªÉ t·∫£i model. Vui l√≤ng ki·ªÉm tra file weights.")
        return

    st.sidebar.success(f"‚úÖ ƒê√£ t·∫£i model: {selected_model}")
    st.sidebar.info(f"üìã S·ªë l∆∞·ª£ng kh√≠a c·∫°nh: {len(aspect_category_names)}")

    # Main content
    st.markdown("---")

    # Mode selection
    analysis_mode = st.radio(
        "Ch·∫ø ƒë·ªô ph√¢n t√≠ch",
        ["Nh·∫≠p m·ªôt review", "Nh·∫≠p nhi·ªÅu reviews"],
        horizontal=True
    )

    if analysis_mode == "Nh·∫≠p m·ªôt review":
        st.markdown("### üìù Nh·∫≠p n·ªôi dung review")

        review_text = st.text_area(
            "Review",
            height=150,
            placeholder="Nh·∫≠p review c·ªßa b·∫°n t·∫°i ƒë√¢y...",
            help="Nh·∫≠p n·ªôi dung ƒë√°nh gi√° v·ªÅ kh√°ch s·∫°n"
        )

        if st.button("üîç Ph√¢n t√≠ch review", type="primary"):
            if not review_text.strip():
                st.warning("Vui l√≤ng nh·∫≠p n·ªôi dung review")
            else:
                with st.spinner("ƒêang ph√¢n t√≠ch review..."):
                    try:
                        results = analyze_single_review(
                            review_text, model, tokenizer,
                            vn_preprocessor, aspect_category_names
                        )

                        st.markdown("---")
                        display_single_analysis_results(results)

                    except Exception as e:
                        st.error(f"L·ªói khi ph√¢n t√≠ch: {str(e)}")
                        st.exception(e)

    else:  # Nh·∫≠p nhi·ªÅu reviews
        st.markdown("### üìã Nh·∫≠p nhi·ªÅu reviews")

        # Option 1: Text area with multiple lines
        reviews_text = st.text_area(
            "Nh·∫≠p c√°c reviews (m·ªói review m·ªôt d√≤ng)",
            height=200,
            placeholder="Review 1...\nReview 2...\nReview 3...",
            help="M·ªói d√≤ng l√† m·ªôt review ri√™ng bi·ªát"
        )

        # Option 2: File upload
        uploaded_file = st.file_uploader(
            "Ho·∫∑c upload file CSV/TXT",
            type=['csv', 'txt'],
            help="File CSV c·∫ßn c√≥ c·ªôt 'review' ho·∫∑c file TXT m·ªói d√≤ng l√† m·ªôt review"
        )

        if st.button("üîç Ph√¢n t√≠ch t·∫•t c·∫£ reviews", type="primary"):
            reviews_list = []

            # Get reviews from text area
            if reviews_text.strip():
                reviews_list = [r.strip() for r in reviews_text.split('\n') if r.strip()]

            # Get reviews from file
            if uploaded_file:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                    if 'review' in df.columns:
                        reviews_list.extend(df['review'].dropna().tolist())
                    else:
                        st.error("File CSV c·∫ßn c√≥ c·ªôt 'review'")
                else:  # txt file
                    content = uploaded_file.read().decode('utf-8')
                    reviews_list.extend([r.strip() for r in content.split('\n') if r.strip()])

            if not reviews_list:
                st.warning("Kh√¥ng c√≥ review n√†o ƒë·ªÉ ph√¢n t√≠ch")
            else:
                st.info(f"T√¨m th·∫•y {len(reviews_list)} reviews")

                # Analyze all reviews
                all_results = []
                progress_bar = st.progress(0)

                for idx, review in enumerate(reviews_list):
                    try:
                        result = analyze_single_review(
                            review, model, tokenizer,
                            vn_preprocessor, aspect_category_names
                        )
                        result['review_text'] = review
                        result['review_id'] = idx + 1
                        all_results.append(result)
                    except Exception as e:
                        st.warning(f"L·ªói khi ph√¢n t√≠ch review {idx+1}: {str(e)}")

                    progress_bar.progress((idx + 1) / len(reviews_list))

                progress_bar.empty()

                # Display summary
                st.markdown("---")
                st.markdown("### üìä T·ªïng quan k·∫øt qu·∫£")

                total_positive = sum(1 for r in all_results if r['overall_sentiment'] == 'T√≠ch c·ª±c')
                total_negative = sum(1 for r in all_results if r['overall_sentiment'] == 'Ti√™u c·ª±c')
                total_neutral = sum(1 for r in all_results if r['overall_sentiment'] == 'Trung b√¨nh')

                col1, col2, col3, col4 = st.columns(4)
                col1.metric("T·ªïng reviews", len(all_results))
                col2.metric("‚úÖ T√≠ch c·ª±c", total_positive)
                col3.metric("‚ùå Ti√™u c·ª±c", total_negative)
                col4.metric("‚ûñ Trung b√¨nh", total_neutral)

                # Show individual results
                st.markdown("### üìã Chi ti·∫øt t·ª´ng review")

                for result in all_results:
                    with st.expander(f"Review #{result['review_id']}: {result['overall_sentiment']}", expanded=False):
                        st.write(f"**N·ªôi dung:** {result['review_text'][:200]}...")
                        # Use use_expander=False to avoid nested expanders
                        display_single_analysis_results(result, use_expander=False)

if __name__ == "__main__":
    main()
